# Week 6

（之前第五周是语法教学，没什么可写的）

## 逻辑回归（分类问题）

这周讲了监督学习中另一类问题——分类问题。

这次的引入例子是肿瘤良性还是恶性的问题。

### 假设

如果依旧按照之前的线性假设，由于我们的结果只有0和1两种情况，并不是很理想，所以我们需要一种新的假设函数。

这里我们采用的是sigmoid函数。

![](http://latex.codecogs.com/gif.latex?g%28z%29%3D%5Cfrac%20%7B1%7D%7B1&plus;e%5E%7B-z%7D%7D)

其中

![](http://latex.codecogs.com/gif.latex?h_%7B%5Ctheta%7D%28x%29%20%3D%20g%28%5Ctheta%5E%7BT%7DX%29)

#### 为什么选定了sigmoid函数

这个渊源得追溯到GLM，不知道学完概统会不会好一点。

知乎：[为什么 LR 模型要使用 sigmoid 函数，背后的数学原理是什么？](https://www.zhihu.com/question/35322351)

讲义：[CS229-notes1](http://cs229.stanford.edu/notes/cs229-notes1.pdf)（重点看GLM部分）

参考：[CSDN](http://blog.csdn.net/u011467621/article/details/48197943)

具体可以参见[GLM笔记](./GLM.md)

### 代价

由于假设函数变化了，如果再使用之前的平方代价模型，会导致局部最优非全局最优（凸凹性）。

所以选择另一个代价函数模型

![](http://latex.codecogs.com/gif.latex?J%28%5Ctheta%29%20%3D%20-%5Cfrac%20%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7Dy%5E%7B%28i%29%7Dlog%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29%29&plus;%281-y%5E%7B%28i%29%7D%29log%281-h_%5Ctheta%28x%5E%7B%28i%29%7D%29)

### 梯度下降

梯度下降的实现跟前面没有区别。

另外比较巧的是上述代价函数的偏导和之前线性回归的形式完全一致，方便梯度下降。

### 决策边界

如字面意思，就是0和1的分界线。

这个边界形状并不固定，跟实际训练结果有关。

### 多类

实际上很多时候并不是简单的{0,1}，而是多个类的分类问题。

解决方法也很简单，假设一共有m(m>2)类，取每一个类看做是1，其他所有其他类是0，这样可以得到m条决策边界，也就是m个h(x)。

然后对于输入的要预测的x0，选取m个h(x)中最大的h(x0)对应的那个类，即为所预测的类。