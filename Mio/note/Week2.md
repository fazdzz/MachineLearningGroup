# Week 2

## 梯度下降算法

这个算法是用来解决回归问题的，可以找到代价函数J的局部最小。

这一周主要在讲这个算法，之前虽然听过很多次了，果然还是吴恩达老师讲的透彻。

例子是**根据房屋面积预测房价**。

不过在之前还是回忆下梯度。

### 梯度

#### 定义

（这里以高数学的二元为例，当然可以推广到多元。）

 函数z=f(x,y)在点P(x,y)的梯度，记为

![](http://latex.codecogs.com/gif.latex?gradf%28x%2Cy%29%3D%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20x%7D%5Cvec%20i%20&plus;%20%5Cfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20y%7D%5Cvec%20j)

#### 性质

- 函数在一点的梯度垂直于该点等值面(或等值线)
- **函数在一点的梯度指向函数增大的方向**

有了梯度的基本定义，整个算法思路就有了。

### 假设

设房屋的面积是x，对应的房价为y，为了预测房价，我们需要找到y和x的关系。

所以我们对房价做出假设(hypothesis)

![](http://latex.codecogs.com/gif.latex?h_%7B%5Ctheta%7D%28x%29%3D%5Ctheta_%7B0%7D%20&plus;%20%5Ctheta_%7B1%7Dx)

这里我们自然有疑问：为什么是线性呢？

实际上很多时候是非线性的，这里方便**直观**理解采用了线性。

### 代价

上述函数h(x)只是我们的假设，对于不同的θ取值我们根据这个函数作出的预测准确率也是不同的，那么如何让我们的预测尽可能可靠呢？

这里就要引入代价函数(cost function)，或者可以认为是误差函数，用J表示。

前面已经提到了，J的取值应该仅仅跟θ1,θ2的取值有关，所以J是一个二元函数。

![](http://latex.codecogs.com/gif.latex?J%28%5Ctheta_%7B0%7D%2C%5Ctheta_%7B1%7D%29%20%3D%20%5Cfrac%7B1%7D%7B2m%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%28h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29-y%5E%7B%28i%29%7D%29%5E%7B2%7D)

- m是样本（训练集）数据总量。
- x和y上标i表示第i组数据。
- 平方是避免作差的正负问题。
- 1/m是取平均值，1/2是为了后面求导后形式好看一些。
- 自变量是θ1和θ2

所以我们希望得到的是对于某个θ1和θ2，让**函数J的值最小**。

### 梯度下降

之前我们提到过梯度的概念，这里就是应用的地方。

所以我们有梯度下降公式

![](http://latex.codecogs.com/gif.latex?%5Ctheta_%7Bj%7D%20%3D%20%5Ctheta_%7Bj%7D%20-%20%5Calpha%20%5Cfrac%20%7B%5Cpartial%7D%7B%5Cpartial%20%5Ctheta_%7Bj%7D%7DJ%28%5Ctheta_%7B0%7D%2C%5Ctheta_%7B1%7D%29)

- 其中α是学习率，通俗说就是每次下降的步伐迈多大。
- j的取值为0或1。
- **这个式子本身也可以看作是在做向量运算。**

根据梯度第一个性质，上述这个公式中θ1和θ2应该是同时更新的，这样才能指向函数增大的方向。